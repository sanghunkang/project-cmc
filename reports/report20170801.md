# Report
#### 1 August 2017

# Common Settings
#### Device Specifications
* CPU: Intel(R) Core(TM) i7-6700K CPU
* GPU: NVIDIA GeForce GTX1080 Ti 2x

# Try5
#### Model Architecture
* Inception-v1 + 3FCs with BN
* 7x7 average pooling layer at the end of convolutions were replace by 14x14 average pooling layer to fit in to the network.

#### Dataset Description
* Training Set: 800+800
* Test Set:     200+200
* Resolution:   448x448x3

#### HyperParameters
* Learning Rate: 0.0001
* Batch Size:    64 + 64
* Iteration:     1000

![Perfomance Report for Try5](./img/report20170801_try5.png)

#### Accomplishments
* Found way to fit data that don't fit to the original architecture.

#### Limitations
* Worse performance that training with 224x224 data.
* Learning is rather unstable.
* Because of more extreme pooling at the end of convolution layers, the model doesn't benefit from increased resolution.

#### Evaluation
* A way to utilise increased information is needed.

# Try6
#### Model Architecture
* Inception-v1 + 3FCs with BN
* 7x7 average pooling layer is kept.
* The first FC layer receives 4x more variables.

#### Dataset Description
* Training Set: 800+800
* Test Set:     200+200
* Resolution:   448x448x3

#### HyperParameters
* Learning Rate: 0.0001
* Batch Size:    64 + 64
* Iteration:     1000

![Perfomance Report for Try6](./img/report20170801_try6.png)

#### Accomplishments
* Better performance, approx 0.10 better than Try4.
* Stabilised learning, especially after convergence.
* Found that increased resolution - i.e. better dataset - contributes to better learning.

#### Limitations
* Expensive computation - approx. 1700 sec, compared to approx. 500 sec. with 224x224 data.

#### Evaluation
* We need to figure out better way of parallel processing, since even if it is confirmed that better dataset brings out better performance, we cannot increase the data resoultion infinitely with knowledge available for now, due to limitation on computation capacity.

# Try7
Inspired by the task J.Choi is doing.

#### Model Architecture
* Inception-v1 + 3FCs with BN

#### Dataset Description
* Training Set: 400+400 
* Test Set:     100+100
* Resolution:   224x224x3
* Only lungs are segmented

#### HyperParameters
* Learning Rate: 0.0001
* Batch Size:    64 + 64
* Iteration:     1000

![Perfomance Report for Try7a](./img/report20170801_try7a.png)
![Perfomance Report for Try7b](./img/report20170801_try7b.png)

#### Accomplishments
* Mediocre. Worked, but the outcome was not so differentiating.

#### Limitations
* Performance is not much different from tries with unsegmented dataset.
* The performance curve shows a similar behaviour as Try3.

#### Evaluation
* There seems to be room for improvement by tuning hyperparameters.
* We need to test whether segmentation contributes to improvement in performance. If it turns out to be so, research on more sophisticated segmentation would be worth doing.

# On Pipeline Implementation
* We were already doing learning on parameters of the convolution network.
* Implementing pipeline method will worsen the performance.

# On Parallelisation
* Computation speed doesn't seem to improve much.
* Memory capacity is expanded, so that we can try bigger models when needed.
* TensorFlow doesn't seem to support a handy API to merge gradients.