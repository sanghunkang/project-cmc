## Project CMC
[한국어 README](./README.ko.md)

## Dependencies
Ubuntu 16.04 LTS<br>
Python 3.5.2<br>

CUDA 8.0.61<br>
CuDNN 8.0<br>

matplotlib 2.1.0<br>
numpy 1.13.3<br>
openCV 3.1.2 (?)<br>
pillow 3.1.2<br>
tensorFlow 1.3.0<br>

Deployments in Windows and OSX are not supported yet.

## Usage
It follows the general convention of CLI programmes. Should you have any question on the detailed usage of each script, `python3 [scriptname].py -h` will print a kind help message in most cases.

## Data Augmentation
*yet to be implemented*

## Data Generataion
We rely on pickles of numpy.ndarray for now. 

* Example
```
python3 datagen.py /
-s /dev-root/class-N_train /
-d /dev-root/diretory-for-train-data /
-r 448 /
-f class_0123_train /
-e 1 /
-n 2
```
Where all the classes except for normal


If you want a random separation of train and validation sets, you 
```
python3 datagen.py -d ... 
python3 splitdata.py -r 0.8 -f /dev-root/ -d /dev-root/ 
```
The first command wi
The second command shuffles the dataset generated by the first command. Then it splits the generated data into a training and a validation dataset, with 80% of the original data used as the training set. 

## Training

* Example
```
python3 train.py /
--dir_data_train=/dev-root/dev-data/diretory-for-train-data /
--dir_data_eval=/dev-root/dev-data/directory-for-validation-data /
--batch_size=128 /
--ckpt_name=checkpoint_my-model /
--num_steps=1000 /
--first_gpu_id=0 /
--num_gpu=2 /
--learning_rate=0.0001
```
This command will trigger the script `train.py` to use files at `/dev-root/dev-data/diretory-for-train-data` as training files and `/dev-root/dev-data/directory-for-validation-data` as validation files. The batch size at each iteration will be 128, and the ratio among classes will be balanced - which is not controllable by system arguments. The result will be stored in the checkpoint file named `checkpoint_my-model.ckpt`. The network will undergo 1000 iteration of weight updates with the learning rate of 0.0001. For computation, the programme will use 2 GPUs from the allocated ID of 0 by ascending order.

Specifically, if you are using the above command as it is, the directory structure of datasets should look more or less like the following:
```
/dev-root/dev-data
	/diretory-for-train-data
		/class-0_train.pickle
		/class-1_train.pickle
	/diretory-for-validation-data
		/class-0_eval.pickle
		/class-1_eval.pickle
```

## Inference

* Example
```
python3 inference.py /
--dir_data_inference=/dev-root/dev-data/directory-for-validation-data /
--ckpt_name= /
```
Note


In addition, it wil... so ...

## Data Encoding
All images are encoded in the format with `A_BC_DDDDDD.bmp`. 

A is the most precise of   one of `X 0 S 1 2 3`. `0` indicates that a patient is perfectly fine. `1`, `2`, `3` indicate that the patient is in the stage 1, 2 or 3 respectively. `S` o  
For now, if an image is from (1) KoSDI or (2) is encoded by Dr.Prof.Myong, `A` is marked with one of `0 S 1 2 3`. Otherwise, it is marked with `X`, which indicates no confirmed diagnosis is available.
If A encoding, it can be said that the validation is reliable.

If `BC` is `00`, `datagen.py` regards it as perfectly normal and label as `N`. Otherwise `B` is labelled as  . For example, `01` is labelled as `0`, and `32` is labelled as `3`.

`DDDDDD` is either a patient ID or KoSDI ID. This is only for identification purpose among the dataset.