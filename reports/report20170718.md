# Report
#### 17 July 2017

# Common Settings
#### Device Specifications
* CPU: Intel(R) Core(TM) i7-6700K CPU
* GPU: NVIDIA GeForce GTX1080 Ti x2

# Some Report on Tes



# Try1
#### Dataset Description
* Training Set: 800+800
* Test Set:     200+200
* Resolution:   448x448x3

#### Model Architecture
* Inception-v1 + 3FCs with BN(batch normalisation) (Ioffe & Szegedy, 2015)
* 7x7 average pooling layer at the end of convolutions were replace by 14x14 average pooling layer to fit in to the network.

#### HyperParameters
* Learning Rate: 0.0001
* Batch Size:    64 + 64
* Iteration:     1000

(Note: Batch size was set to 16 to fit the available GPU memoery capacity.)<br/>

![Perfomance Report for Try1](./img/report20170717_try1.png)

#### Accomplishments


#####

# Try2
#### Dataset Description
* Training Set: 800+800
* Test Set:     200+200
* Resolution:   448x448x3

#### Model Architecture
* Inception-v1 + 3FCs with BN(batch normalisation) (Ioffe & Szegedy, 2015)
* 7x7 average pooling layer at the end of convolutions were replace by 14x14 average pooling layer to fit in to the network.

#### HyperParameters
* Learning Rate: 0.0001
* Batch Size:    64 + 64
* Iteration:     1000





# Try3
####
Inspired by the task J.Choi is doing.

#### Dataset Description
* Training Set: 800+800
* Test Set:     200+200
* Resolution:   448x448x3