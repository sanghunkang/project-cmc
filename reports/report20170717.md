# Report
#### 17 July 2017

# Common Settings
#### Device Specifications
* CPU: Intel i7-...{{{Model Name}}}
* GPU: NVIDIA GTX970 x1
* Tensorflow 1.2.1

#### Dataset Description
* Training Set: 800+800
* Test Set:     200+200
* Resolution:   224x224x3

# Try1
#### Model Architecture
* VGG16 {{{Citation}}}

#### HyperParameters
* Learning Rate: 0.001
* Batch Size:    16
* Iteration:     1000

(note: Batch size was set to 16 to fit the available GPU memoery capacity.)<br/>

#### Accomplishments
* Nothing, indisputable failure.

#### Limitations
* Accuracy oscillates around 0.5, which is nothing more than simple guess following the distribution of the data.
* Too many parameters, which makes increasing batch size unfeasible.

#### Evaluation
* For now, implementing ResNet architecture cannot help as the number of parameters required is larger than VGG16.
* For the same reason, using Inception-v1 might could be a solution since it allows bigger batch size
* It might be even better if we could obtain pretrained weights of v2, v3 of Inception model.
* Maybe the number of iterations could have been too small.

# Try2
#### Model Architecture
* Inception-v1 + 3FC(fully connected layer)s {{{Citation}}}

(note: The original Inception-v1 architecture doesn't have 3 following full connected layers.)<br/>
#### HyperParameters
* Learning Rate: 0.001	
* Batch Size:    128	
* Iteration:     1000

#### Accomplishments
* It is confirmed that it is possible to run the model with currently available device, as Inception-v1 dramatically reduces the number of parameters required - 1/6 of VGG16.

#### Limitations
* Accuracy still doesn't converge.

#### Evaluation
* It might be even better if we could obtain pretrained weights of v2, v3 of Inception model.
* Still, it is proven that without modifying inputs by any means, learning is impossible.

# Try3
#### Model Architecture
* Inception-v1 + 3FCs with BN(batch normalisation) {{{Citation}}}

#### HyperParameters
* Learning Rate: 0.001	
* Batch Size:    128	
* Iteration:     1000

![Perfomance Report for Try3](/img/report20170717_try3.png)

#### Accomplishments
* Finally showed some evidence that the model can learn.
* Found a way to detour PCA Whitening, which requires huge computation
* ... and dropout algorithm, which slows down learning speed.

#### Limitations
* Although accuracy seems to increase rapidly at the beginning of the iterations, after some point it returns to 0.5 level and oscillates around there.
* It is still unsure whether the learning was possible because the model could really detect the relevant features or just detected co-occuring irrelevant features (e.g. skeletal structure)

#### Evaluation
* It would be worth trying to set the learning rate small, since it seems that the accuracy escapes from the well performing region.
* We need to devise some method to inspect whether the model is really learning relevant features.
* It is also required to further stabilise the model.

# Try4
#### Model Architecture
* Inception-v1 + 3FC with BN

#### HyperParameters
* Learning Rate: 0.0001
* Batch Size:    128
* Iteration:     1000



# 다음시도 ########################################################################


피쳐필터가 완료된 자료를 인풋으로 넣고 학습을 시킬 수도 있겠다.
즉 단순 MLP로 문제를 치환해서 한 3층정도만.
만약에 GPU메모리를 키워야 한다고 쳤을 때 어느정도 필요한지는 모르겠다.

그리고 골격이 문제라면 차라리 비정상집단 안에서의 분류를 먼저 시도하는 편이 어떨까?